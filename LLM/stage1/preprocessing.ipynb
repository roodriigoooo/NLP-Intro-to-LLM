{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-23T20:41:32.756729Z",
     "start_time": "2025-03-23T20:41:32.611232Z"
    }
   },
   "source": [
    "import urllib.request\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "       \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "       \"the-verdict.txt\")\n",
    "file_path = \"the-verdict.txt\"\n",
    "urllib.request.urlretrieve(url, file_path)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the-verdict.txt', <http.client.HTTPMessage at 0x111af5b90>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:41:32.771527Z",
     "start_time": "2025-03-23T20:41:32.766986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "print(\"Total number of characters:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ],
   "id": "ce12567de80a87c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:41:32.887691Z",
     "start_time": "2025-03-23T20:41:32.882077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "print(result)\n",
    "\n",
    "# We refrain from making all text lower-case because caps help LLMs distinguish between proper nouns and common nouns, understand sentence structure, and learn to generate text with proper capitalization."
   ],
   "id": "49674fe19f61b9b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:41:33.061163Z",
     "start_time": "2025-03-23T20:41:33.056196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)\n",
    "\n",
    "#whether or not we encode whitespaces as separate characters or just remove them depends on our application and its requirements.\n",
    "#keeping whitespaces can be useful if we train models that are sensitive to the exact structure of the text (like python code). Removing them reduces memory and computing requirements."
   ],
   "id": "44981cc313b2d1ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:41:33.166124Z",
     "start_time": "2025-03-23T20:41:33.158869Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ],
   "id": "d1121209e8475642",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:41:33.293447Z",
     "start_time": "2025-03-23T20:41:33.284299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(len(preprocessed))\n",
    "print(preprocessed[:30])\n",
    "\n",
    "# vocabs define how we map each unique word and special character to a unique integer. each unique token is mapped to a unique token ID.\n",
    "# each unique token is added to the vocab in alphabetical order. duplicate tokens are removed."
   ],
   "id": "13c61c5767ba9880",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:41:33.346121Z",
     "start_time": "2025-03-23T20:41:33.338647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)\n",
    "\n",
    "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break\n",
    "\n",
    "#starting with a new sample text, we tokenize this new text and use the existing vocabulary to convert the text tokens into token IDs.\n",
    "# the vocab is built from the entire training set and can be applied to the training set itself and any new text samples."
   ],
   "id": "ff4193659cfe27fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n",
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**A complete tokenizer class** that:\n",
    "\n",
    "- Splits text into tokens.\n",
    "- Carries out the string-to-integer mapping to produce token IDs via the vocabulary.\n",
    "- Implements a `decode` method that carries out the reverse integer-to-string mapping to convert the token IDs back into text."
   ],
   "id": "58570e70074eca2c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:41:33.406065Z",
     "start_time": "2025-03-23T20:41:33.398103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab): # the function works via an existing vocab, which we can use to encode and decode text.\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text) # preprocesses input text into token IDs\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids]) # converts tokens back into text\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text) #removes spaces before the specified punctuation\n",
    "        return text\n",
    "\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"It's the last he painted, you know, Mrs.Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n",
    "print(tokenizer.decode(ids))"
   ],
   "id": "d712a004ca4cae7f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 67, 7, 38, 851, 1108, 754, 793, 7]\n",
      "It' s the last he painted, you know, Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:41:33.773682Z",
     "start_time": "2025-03-23T20:41:33.471202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"Hello, do you like tea?\"\n",
    "print(tokenizer.encode(text)) #word \"Hello\" not included in \"The Verdict\" short story. Need to consider larger, diverse training sets to extend the vocabulary when working with LLMs.\n",
    "# we can just modify the tokenizer to use an <|unk|> token if it encounters a word that is not part of the vocabulary.\n",
    "# we can also add a <|endoftext|> token that we can use to separate two unrelated text sources. this helps the lLM understand that although these text sources are concatenated for training, they are, in fact, unrelated."
   ],
   "id": "7c9907b50b2f972a",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHello, do you like tea?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m) \u001B[38;5;66;03m#word \"Hello\" not included in \"The Verdict\" short story. Need to consider larger, diverse training sets to extend the vocabulary when working with LLMs.\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# we can just modify the tokenizer to use an <|unk|> token if it encounters a word that is not part of the vocabulary.\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# we can also add a <|endoftext|> token that we can use to separate two unrelated text sources. this helps the lLM understand that although these text sources are concatenated for training, they are, in fact, unrelated.\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[12], line 9\u001B[0m, in \u001B[0;36mSimpleTokenizerV1.encode\u001B[0;34m(self, text)\u001B[0m\n\u001B[1;32m      7\u001B[0m preprocessed \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m([,.:;?_!\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m()\u001B[39m\u001B[38;5;130;01m\\'\u001B[39;00m\u001B[38;5;124m]|--|\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124ms)\u001B[39m\u001B[38;5;124m'\u001B[39m, text) \u001B[38;5;66;03m# preprocesses input text into token IDs\u001B[39;00m\n\u001B[1;32m      8\u001B[0m preprocessed \u001B[38;5;241m=\u001B[39m [item\u001B[38;5;241m.\u001B[39mstrip() \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m preprocessed \u001B[38;5;28;01mif\u001B[39;00m item\u001B[38;5;241m.\u001B[39mstrip()]\n\u001B[0;32m----> 9\u001B[0m ids \u001B[38;5;241m=\u001B[39m \u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstr_to_int\u001B[49m\u001B[43m[\u001B[49m\u001B[43ms\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43ms\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpreprocessed\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ids\n",
      "Cell \u001B[0;32mIn[12], line 9\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m      7\u001B[0m preprocessed \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m([,.:;?_!\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m()\u001B[39m\u001B[38;5;130;01m\\'\u001B[39;00m\u001B[38;5;124m]|--|\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124ms)\u001B[39m\u001B[38;5;124m'\u001B[39m, text) \u001B[38;5;66;03m# preprocesses input text into token IDs\u001B[39;00m\n\u001B[1;32m      8\u001B[0m preprocessed \u001B[38;5;241m=\u001B[39m [item\u001B[38;5;241m.\u001B[39mstrip() \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m preprocessed \u001B[38;5;28;01mif\u001B[39;00m item\u001B[38;5;241m.\u001B[39mstrip()]\n\u001B[0;32m----> 9\u001B[0m ids \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstr_to_int\u001B[49m\u001B[43m[\u001B[49m\u001B[43ms\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m preprocessed]\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ids\n",
      "\u001B[0;31mKeyError\u001B[0m: 'Hello'"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:41:41.330280Z",
     "start_time": "2025-03-23T20:41:41.324754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token:integer for integer, token in enumerate(all_tokens)}\n",
    "print(len(vocab.items()))"
   ],
   "id": "b8bfbc422497020e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:41:42.900061Z",
     "start_time": "2025-03-23T20:41:42.891933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab): # the function works via an existing vocab, which we can use to encode and decode text.\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text) # preprocesses input text into token IDs\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids]) # converts tokens back into text\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text) #removes spaces before the specified punctuation\n",
    "        return text\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)\n",
    "\n",
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "print(tokenizer.encode(text))\n",
    "print(tokenizer.decode(tokenizer.encode(text))) #quick sanity check"
   ],
   "id": "6ad78930052238ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
      "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n",
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Depending on the LLM, some researchers also consider additional special tokens such as the following:\n",
    "\n",
    "- `[BOS]` (beginning of sequence), marks the start of a text. Signifies to the LLM where a piece of content begins.\n",
    "- `[EOS]` (end of sequence), positioned at the end of a text and is especially useful when concatenating multiple unrelated texts, similar to `<|endoftext|>`.\n",
    "- `[PAD]` (padding), when training LLMs with batch sizes larger than one, the batch might contain texts of varying length. The shorter texts are extended, padded, up to the length of the longest text in the batch.\n"
   ],
   "id": "c84424da459ca1f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:48:28.495125Z",
     "start_time": "2025-03-23T20:48:28.489835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "\n",
    "#bpe breaks down words into subword units. Unknown tokens are not longer used, nor needed.\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text = (\"Helo, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "        \"of someunknownPlace\")\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)\n",
    "\n",
    "#the endoftext token is assigned the largest token ID, out of 50257 tokens.\n",
    "#the BPE encodes and decodes unknown words correctly."
   ],
   "id": "8f37fadc2d8f8ab1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12621, 78, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271]\n",
      "Helo, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The algorithm underlying byte-pair encoding breaks down words that aren't in its predefined vocabulary into smaller subwords units or even individual characters, enabling it to handle out-of-vocab words.\n",
    "BPE builds its vocabulary iteratively merging frequent characters into subwords and frequent subwords into words. It starts by adding all individual single characters to its vocab (\"a\", \"b\", etc.). Next, it merges character combinations that frequently occur together into subwords, each merge being determined by a frequency cutoff.\n",
    "\n",
    "**Example:**\n",
    "- The text sample \"Akwir ier\" would first be tokenized into individual characters or subwords.\n",
    "- \"Ak\", \"w\", \"ir\", \"w\", \"\", \"ier\", each with their corresponding token IDs (33901, 86, 343, 86, 220, 959).\n",
    "\n",
    "The ability to break down words into individual characters ensures that the tokenizer, and the LLM that is trained with it, can process any text, even if it contains words that were not present in its training data."
   ],
   "id": "998b2a87959bd8ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:58:02.961936Z",
     "start_time": "2025-03-23T20:58:02.957582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "example = \"Awkir ier\"\n",
    "integers_ex = tokenizer.encode(example, allowed_special={\"<|endoftext|>\"})\n",
    "string_ex = tokenizer.decode(integers_ex)\n",
    "print(integers_ex, string_ex)"
   ],
   "id": "64c9efb2f624b334",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23155, 74, 343, 220, 959] Awkir ier\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Data Sampling with a Sliding Window\n",
    "We now create the input-target pairs required for training the LLM. LLMs are pretrained by predicting the next word in a text, and we mask out all words that are past the target.\n",
    "**Example:**\n",
    "\n",
    "- Input: \"LLMs\", target: \"learn\"\n",
    "- Input: \"LLMs learn\", target: \"to\"\n",
    "- Input: \"LLMs learn to\", target: \"predict\"\n",
    "- Input: \"LLMs learn to predict\", target: \"one\"\n",
    "- Input: \"LLMs learn to predict one\", target: \"word\"\n"
   ],
   "id": "f98c9c08576d2586"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T21:07:14.331029Z",
     "start_time": "2025-03-23T21:07:14.321073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))\n",
    "\n",
    "enc_sample = enc_text[50:]\n",
    "context_size = 4 #how many tokens are included in the input\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ],
   "id": "6bc2ce86cb8426be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n",
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T21:10:18.243748Z",
     "start_time": "2025-03-23T21:10:18.238240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context, \"--->\", desired)\n",
    "\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"--->\", tokenizer.decode([desired]))"
   ],
   "id": "59e14c2c8dad01ed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ---> 4920\n",
      "[290, 4920] ---> 2241\n",
      "[290, 4920, 2241] ---> 287\n",
      "[290, 4920, 2241, 287] ---> 257\n",
      " and --->  established\n",
      " and established --->  himself\n",
      " and established himself --->  in\n",
      " and established himself in --->  a\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As a last step to enable embeddings, we need an efficient data loader that iterates over the input dataset and returns the inputs and targets as PyTorch tensors. We want to return two tensors:\n",
    "\n",
    "- an input tensor containing the text that the LLM sees.\n",
    "- an output, target sensor that includes the targets for the LLM to predict.\n",
    "\n",
    "Each row in the (of size `max_length`) input tensor `x` represents one input context. A second tensor,`y` contains the corresponding prediction targets, which are created by shifting the input by one position."
   ],
   "id": "9ae7f7401c97dfef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T21:26:39.413386Z",
     "start_time": "2025-03-23T21:26:39.404041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        token_ids = tokenizer.encode(txt) #tokenizes entire text, and converts them into token IDs as a single step.\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk)) # use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids) # returns the total number of row in the dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx] #return a single row from dataset.\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDataset(txt, tokenizer, max_length=max_length, stride=stride)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle,\n",
    "                            drop_last=drop_last, #drop_last=True drops the last batch if it is shorter than the specified batch_size to prevent loss spikes during training.\n",
    "                            num_workers=num_workers) # num of CPU processes to use for preprocessing.\n",
    "    return dataloader"
   ],
   "id": "4e6a052d05ab5188",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We test the `dataloader` with a batch size of 1 for an LLM with a context size of 4 to develop an intuition of how the `GPTDatasetV1` class and the `create_dataloader_v1` work together.",
   "id": "7f1a89c449f4f2a7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T21:29:13.400868Z",
     "start_time": "2025-03-23T21:29:13.305376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "data_iter = iter(dataloader) # convert dataloader into a iterator to fetch the next entry, leveraging the defined __getitem__ function above.\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch[0]) #the input token IDs\n",
    "print(first_batch[1]) #the target token IDs.\n",
    "\n",
    "#Since max_length is set to 4, each of the two tensors contain four token IDs."
   ],
   "id": "641add05822f0e16",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  40,  367, 2885, 1464]])\n",
      "tensor([[ 367, 2885, 1464]])\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T21:30:01.312391Z",
     "start_time": "2025-03-23T21:30:01.307255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch[0])\n",
    "print(second_batch[1])"
   ],
   "id": "d354227c6c045ea2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 367, 2885, 1464, 1807]])\n",
      "tensor([[2885, 1464, 1807]])\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "If we compare the first and second batches, we see that the second batch's token IDs are shifted by only one positions. The `stride` parameter dictates the number of positions the inputs shift across batches, emulating a sliding window approach.\n",
   "id": "1a32237952256163"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "173caca32a01af4d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
